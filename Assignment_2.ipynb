{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a091d71",
   "metadata": {},
   "source": [
    "# ***NLP Assignment 2***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0670661d",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b76076",
   "metadata": {},
   "source": [
    "### *Mention and define the NLTK steps.*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5516990c",
   "metadata": {},
   "source": [
    "`Initial Steps`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15059301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Python 3.5 reached the end of its life on September 13th, 2020. Please upgrade your Python as Python 3.5 is no longer maintained. pip 21.0 will drop support for Python 3.5 in January 2021. pip 21.0 will remove support for this functionality.\u001b[0m\n",
      "Requirement already satisfied: nltk in /Users/amansharma/opt/anaconda3/envs/ds/lib/python3.5/site-packages (3.6.2)\n",
      "Requirement already satisfied: regex in /Users/amansharma/opt/anaconda3/envs/ds/lib/python3.5/site-packages (from nltk) (2022.1.18)\n",
      "Requirement already satisfied: joblib in /Users/amansharma/opt/anaconda3/envs/ds/lib/python3.5/site-packages (from nltk) (0.14.1)\n",
      "Requirement already satisfied: click in /Users/amansharma/opt/anaconda3/envs/ds/lib/python3.5/site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: tqdm in /Users/amansharma/opt/anaconda3/envs/ds/lib/python3.5/site-packages (from nltk) (4.63.0)\n",
      "Requirement already satisfied: importlib-resources in /Users/amansharma/opt/anaconda3/envs/ds/lib/python3.5/site-packages (from tqdm->nltk) (3.2.1)\n",
      "Requirement already satisfied: zipp>=0.4 in /Users/amansharma/opt/anaconda3/envs/ds/lib/python3.5/site-packages (from importlib-resources->tqdm->nltk) (1.2.0)\n"
     ]
    }
   ],
   "source": [
    "# Installation of NLTK\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ee1975c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import NLTK\n",
    "import nltk "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3038b761",
   "metadata": {},
   "source": [
    "`Downloading the datasets (optional)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eccb090f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d0393f",
   "metadata": {},
   "source": [
    "Import the required dataset, which can be stored and accessed locally or online through a web URL. We can also make use of one of the corpus datasets provided by NLTK itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7474b1fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fellow-Citizens of the Senate and of the House of Representatives:\n",
      "\n",
      "Among the vicissitudes incident to life no event could have filled me with greater anxieties than that of which the notification was transmitted by your order, and received on the 14th day of the present month. On the one hand, I was summoned by my Country, whose voice I can never hear but with veneration and love, from a retreat which I had chosen with the fondest predilection, and, in my flattering hopes, with an immutable dec\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import inaugural\n",
    "corpus = inaugural.raw('1789-Washington.txt')\n",
    "#print head of corpus\n",
    "print(corpus[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b40f7d9",
   "metadata": {},
   "source": [
    "### ***`Tokenization`***\n",
    "Tokenization is the first step in text analytics. The process of breaking down a text paragraph into smaller chunks such as words or sentence is called Tokenization. Token is a single entity that is building blocks for sentence or paragraph."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74bea52",
   "metadata": {},
   "source": [
    "*Sentence Tokenization*\n",
    "\n",
    "Sentence tokenizer breaks text paragraph into sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4a7b22ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of sentences is 23\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "sents = nltk.sent_tokenize(corpus)\n",
    "print(\"The number of sentences is\", len(sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6758812e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Fellow-Citizens of the Senate and of the House of Representatives:\\n\\nAmong the vicissitudes incident to life no event could have filled me with greater anxieties than that of which the notification was transmitted by your order, and received on the 14th day of the present month.', 'On the one hand, I was summoned by my Country, whose voice I can never hear but with veneration and love, from a retreat which I had chosen with the fondest predilection, and, in my flattering hopes, with an immutable decision, as the asylum of my declining years -- a retreat which was rendered every day more necessary as well as more dear to me by the addition of habit to inclination, and of frequent interruptions in my health to the gradual waste committed on it by time.', 'On the other hand, the magnitude and difficulty of the trust to which the voice of my country called me, being sufficient to awaken in the wisest and most experienced of her citizens a distrustful scrutiny into his qualifications, could not but overwhelm with despondence one who (inheriting inferior endowments from nature and unpracticed in the duties of civil administration) ought to be peculiarly conscious of his own deficiencies.', 'In this conflict of emotions all I dare aver is that it has been my faithful study to collect my duty from a just appreciation of every circumstance by which it might be affected.', 'All I dare hope is that if, in executing this task, I have been too much swayed by a grateful remembrance of former instances, or by an affectionate sensibility to this transcendent proof of the confidence of my fellow citizens, and have thence too little consulted my incapacity as well as disinclination for the weighty and untried cares before me, my error will be palliated by the motives which mislead me, and its consequences be judged by my country with some share of the partiality in which they originated.']\n"
     ]
    }
   ],
   "source": [
    "#print first 5 sentences\n",
    "print (sents[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6f2be1",
   "metadata": {},
   "source": [
    "*Word Tokenization*\n",
    "\n",
    "Word tokenizer breaks text paragraph into words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a37e2250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of tokens is 1537\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "words = nltk.word_tokenize(corpus)\n",
    "print(\"The number of tokens is\", len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "063676ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Fellow-Citizens', 'of', 'the', 'Senate', 'and', 'of', 'the', 'House', 'of', 'Representatives', ':', 'Among', 'the', 'vicissitudes', 'incident', 'to', 'life', 'no', 'event', 'could', 'have', 'filled', 'me', 'with', 'greater', 'anxieties', 'than', 'that', 'of', 'which', 'the', 'notification', 'was', 'transmitted', 'by', 'your', 'order', ',', 'and', 'received', 'on', 'the', '14th', 'day', 'of', 'the', 'present', 'month', '.', 'On', 'the', 'one', 'hand', ',', 'I', 'was', 'summoned', 'by', 'my', 'Country', ',', 'whose', 'voice', 'I', 'can', 'never', 'hear', 'but', 'with', 'veneration', 'and', 'love', ',', 'from', 'a', 'retreat', 'which', 'I', 'had', 'chosen', 'with', 'the', 'fondest', 'predilection', ',', 'and', ',', 'in', 'my', 'flattering', 'hopes', ',', 'with', 'an', 'immutable', 'decision', ',', 'as', 'the', 'asylum']\n"
     ]
    }
   ],
   "source": [
    "#print first 100 words\n",
    "print(words[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12699724",
   "metadata": {},
   "source": [
    "`Stop Words removal`\n",
    "\n",
    "When we use the features from a text to model, we will encounter a lot of noise. These are the stop words like the, he, her, etc… which don’t help us and, just be removed before processing for cleaner processing inside the model. With NLTK we can see all the stop words available in the English language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c3d8f9d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "68243637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of total tokens after removing stopwords are 800\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "final_tokens = []\n",
    "for each in words:\n",
    " if each not in stop_words:\n",
    "    final_tokens.append(each)\n",
    "print(\"The number of total tokens after removing stopwords are\", len((final_tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b646a4",
   "metadata": {},
   "source": [
    "`Stemming`\n",
    "\n",
    "In our text we may find many words like playing, played, playfully, etc… which have a root word, play all of these convey the same meaning. So we can just extract the root word and remove the rest. Here the root word formed is called ‘stem’ and it is not necessarily that stem needs to exist and have a meaning. Just by committing the suffix and prefix, we generate the stems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fc5cdd53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Porter stemmed words:  ['grow', 'leav', 'fairli', 'cat', 'troubl', 'misunderstand', 'friendship', 'easili', 'ration', 'relat']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer_ps = PorterStemmer() \n",
    "words = [\"grows\",\"leaves\",\"fairly\",\"cats\",\"trouble\",\"misunderstanding\",\"friendships\",\"easily\", \"rational\", \"relational\"]\n",
    "words_ps = [stemmer_ps.stem(word) for word in words]\n",
    "print(\"Porter stemmed words: \", words_ps)         "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a1aca6",
   "metadata": {},
   "source": [
    "`Lemmatization`\n",
    "\n",
    "We want to extract the base form of the word here. The word extracted here is called Lemma and it is available in the dictionary. We have the WordNet corpus and the lemma generated will be available in this corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b72a3b4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/amansharma/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The lemmatized words:  ['grows', 'leaf', 'fairly', 'cat', 'trouble', 'running', 'friendship', 'easily', 'wa', 'relational', 'ha']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet') \n",
    "words = [\"grows\",\"leaves\",\"fairly\",\"cats\",\"trouble\",\"running\",\"friendships\",\"easily\", \"was\", \"relational\",\"has\"]\n",
    "lemmatizer = WordNetLemmatizer()   \n",
    "le_words = [lemmatizer.lemmatize(word) for word in words] \n",
    "print(\"The lemmatized words: \", le_words) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1c2e04",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4004767",
   "metadata": {},
   "source": [
    "### What different types of segmentation are there?\n",
    "- Text segmentation\n",
    "- Word segmentation\n",
    "- Intent segmentation\n",
    "- Sentence segmentation\n",
    "- Topic segmentation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef31c2bd",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90cd3f71",
   "metadata": {},
   "source": [
    "### Explain the method of recognising named entities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7610ebb9",
   "metadata": {},
   "source": [
    "Named entity recognition (NER)is probably the first step towards information extraction that seeks to locate and classify named entities in text into pre-defined categories such as the names of persons, organizations, locations, expressions of times, quantities, monetary values, percentages, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fdf11e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "40420377",
   "metadata": {},
   "outputs": [],
   "source": [
    "ex = 'European authorities fined Google a record $5.1 billion on Wednesday for abusing its power in the mobile phone market and ordered the company to alter its practices'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3488cd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(sent):\n",
    "    sent = nltk.word_tokenize(sent)\n",
    "    sent = nltk.pos_tag(sent)\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "33a0b27e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('European', 'JJ'),\n",
       " ('authorities', 'NNS'),\n",
       " ('fined', 'VBD'),\n",
       " ('Google', 'NNP'),\n",
       " ('a', 'DT'),\n",
       " ('record', 'NN'),\n",
       " ('$', '$'),\n",
       " ('5.1', 'CD'),\n",
       " ('billion', 'CD'),\n",
       " ('on', 'IN'),\n",
       " ('Wednesday', 'NNP'),\n",
       " ('for', 'IN'),\n",
       " ('abusing', 'VBG'),\n",
       " ('its', 'PRP$'),\n",
       " ('power', 'NN'),\n",
       " ('in', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('mobile', 'JJ'),\n",
       " ('phone', 'NN'),\n",
       " ('market', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('ordered', 'VBD'),\n",
       " ('the', 'DT'),\n",
       " ('company', 'NN'),\n",
       " ('to', 'TO'),\n",
       " ('alter', 'VB'),\n",
       " ('its', 'PRP$'),\n",
       " ('practices', 'NNS')]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = preprocess(ex)\n",
    "sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c89f985d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = 'NP: {<DT>?<JJ>*<NN>}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d591aca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  European/JJ\n",
      "  authorities/NNS\n",
      "  fined/VBD\n",
      "  Google/NNP\n",
      "  (NP a/DT record/NN)\n",
      "  $/$\n",
      "  5.1/CD\n",
      "  billion/CD\n",
      "  on/IN\n",
      "  Wednesday/NNP\n",
      "  for/IN\n",
      "  abusing/VBG\n",
      "  its/PRP$\n",
      "  (NP power/NN)\n",
      "  in/IN\n",
      "  (NP the/DT mobile/JJ phone/NN)\n",
      "  (NP market/NN)\n",
      "  and/CC\n",
      "  ordered/VBD\n",
      "  (NP the/DT company/NN)\n",
      "  to/TO\n",
      "  alter/VB\n",
      "  its/PRP$\n",
      "  practices/NNS)\n"
     ]
    }
   ],
   "source": [
    "cp = nltk.RegexpParser(pattern)\n",
    "cs = cp.parse(sent)\n",
    "print(cs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86483a73",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae190a0",
   "metadata": {},
   "source": [
    "### Make a list of the elements of NLP.\n",
    "\n",
    "- `Entity extraction` --> Entity extraction involves segmenting a sentence to identify and extract entities, such as a person (real or fictional), organization, geographies, events, etc. \n",
    "- `Syntactic analysis` ---> Syntax refers to the proper ordering of words\n",
    "- `Semantic Analysis` ---> Semantic analysis concludes the meaning of the sentence in a context-free form as an independent sentence.\n",
    "- `Sentiment Analysis` ---> Sentiment will include emotions, opinions, and attitudes. We are talking subjective impressions and not facts\n",
    "- `Pragmatic analysis` ---> Pragmatic analysis uses the context of utterance—when, why, by who, where, to whom something was said. It deals with intentions like criticize, inform, promise, request, and so on"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48075e2",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e95001",
   "metadata": {},
   "source": [
    "### Give an example of pragmatic analysis.\n",
    "Pragmatic analysis in NLP can be defined as the set of logical and linguistic tools with the help of which analysts can develop systematic accounts of discursive political interactions. They attempt to classify the full variety of the inferences that any hearer or reader can make when encountering the locations of the author or speaker.\n",
    "\n",
    "E.g., \"close the window?\" should be interpreted as a request instead of an order."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e4809a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c02b73c",
   "metadata": {},
   "source": [
    "### Explain the morphological and lexical analysis procedure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ff71fb",
   "metadata": {},
   "source": [
    "`Morphological analysis`\n",
    "- Morphological analysis or general morphological analysis, a method for exploring all possible solutions to a multi-dimensional, non-quantified problem\n",
    "- Analysis of morphology , the internal structure of words. Morphological parsing is conducted by computers to extract morphological information from a given wordform."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b153d96",
   "metadata": {},
   "source": [
    "`Lexical analysis` \n",
    "- Process of taking an input string of characters (such as the source code of a computer program) and producing a sequence of symbols called lexical tokens, or just tokens, which may be handled more easily by a parser."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c80bba",
   "metadata": {},
   "source": [
    "----\n",
    "----"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
