{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Assignment_5.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPtyDWIzrkmaVo1mcm3VjfM"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **NLP Assignment 5**"],"metadata":{"id":"FMIpAYjAPWa1"}},{"cell_type":"markdown","source":["---\n","---"],"metadata":{"id":"t8lw9PlfPYk5"}},{"cell_type":"markdown","source":["## **Explain the concept of lemmatization.**"],"metadata":{"id":"v8AcA4pZPZwR"}},{"cell_type":"markdown","source":["`Lemmatization` is a linguistic term that means grouping together words with the same root or lemma but with different inflections or derivatives of meaning so they can be analyzed as one item. The aim is to take away inflectional suffixes and prefixes to bring out the word’s dictionary form.\n","\n","For example, to lemmatize the words “cats,” “cat’s,” and “cats’” means taking away the suffixes “s,” “’s,” and “s’” to bring out the root word “cat.” Lemmatization is used to train robots to speak and converse, making it important in the field of artificial intelligence (AI) known as “natural language processing (NLP)” or “natural language understanding.”"],"metadata":{"id":"W40BRTXvPgbq"}},{"cell_type":"markdown","source":["---"],"metadata":{"id":"Hf7J7ik3Pmh5"}},{"cell_type":"markdown","source":["## **What exactly is NLU? Mention how it's used.**"],"metadata":{"id":"Ck28hXFkPnXr"}},{"cell_type":"markdown","source":["`NLU - Natural Language Understanding` is branch of natural language processing (NLP), which helps computers understand and interpret human language by breaking down the elemental pieces of speech. While speech recognition captures spoken language in real-time, transcribes it, and returns text, NLU goes beyond recognition to determine a user’s intent. Speech recognition is powered by statistical machine learning methods which add numeric structure to large datasets. In NLU, machine learning models improve over time as they learn to recognize syntax, context, language patterns, unique definitions, sentiment, and intent.\n","\n","`Steps of Natural Language Understanding`\n"," \n","According to the traditional system there are three steps in natural language understanding. Every step focuses on “semantics” of the language.\n","\n"," \n","\n","- Step1  (Language in Context):- This is the first step of understanding in NLU. At this step the focus is on the meaning of dialogue and discourse. The system focuses on facilitating the conversation between a voicebot and a human being.\n","\n","- Step2  (Compositional Semantics):- The second step of NLU focuses on “compositional semantics”, it constructs the meaning of the sentence based on its syntax. \n","\n","- Step 3  (Lexical Semantics):- The highest level of Natural Language Understanding focuses on understanding the meaning of words. It focuses on deriving the meaning of individual words from sentences. \n","\n"],"metadata":{"id":"yGroiu_rP7zl"}},{"cell_type":"markdown","source":["---"],"metadata":{"id":"qspeZJoLQT4L"}},{"cell_type":"markdown","source":["## What exactly are stop words?"],"metadata":{"id":"V7O_ZTptQYYl"}},{"cell_type":"markdown","source":["`Stopwords` are the words in any language which does not add much meaning to a sentence. They can safely be ignored without sacrificing the meaning of the sentence. For some search engines, these are some of the most common, short function words, such as the, is, at, which, and on. In this case, stop words can cause problems when searching for phrases that include them, particularly in names such as “The Who” or “Take That”."],"metadata":{"id":"kgkP9Q7BQaSE"}},{"cell_type":"markdown","source":["---"],"metadata":{"id":"v2JTYppwQoxG"}},{"cell_type":"markdown","source":["## What is corpus juris?"],"metadata":{"id":"vbbbVbgdQ7HW"}},{"cell_type":"markdown","source":["\n","`body of law`\n","Corpus juris is Latin for “body of law.” It may also be the title of a large, encyclopedic collection of laws, comprising an entire body of law"],"metadata":{"id":"6biYzEu8Q29W"}},{"cell_type":"markdown","source":["---"],"metadata":{"id":"P7RishE9Q-UC"}},{"cell_type":"markdown","source":["## Describe the veiled language model."],"metadata":{"id":"VXl-VC7yRCK1"}},{"cell_type":"markdown","source":["A language model is the core component of modern Natural Language Processing (NLP). It’s a statistical tool that analyzes the pattern of human language for the prediction of words. \n","\n","NLP-based applications use language models for a variety of tasks, such as audio to text conversion, speech recognition, sentiment analysis, summarization, spell correction, etc. \n","\n","Let’s understand how language models help in processing these NLP tasks: \n","\n","- Speech Recognition: Smart speakers, such as Alexa, use automatic speech recognition (ASR) mechanisms for translating the speech into text. It translates the spoken words into text and between this translation, the ASR mechanism analyzes the intent/sentiments of the user by differentiating between the words. For example, analyzing homophone phrases such as “Let her” or “Letter”, “But her” “Butter”.\n","\n","- Machine Translation: When translating a Chinese phrase into English, the translator can give several choices \n"],"metadata":{"id":"E6kmCPhuRD_D"}},{"cell_type":"markdown","source":["---"],"metadata":{"id":"qQ0DBMmyRmtP"}},{"cell_type":"markdown","source":["## Explain how a word embedding works."],"metadata":{"id":"eE2raedaRjIk"}},{"cell_type":"markdown","source":["Word Embeddings are a method of extracting features out of text so that we can input those features into a machine learning model to work with text data. They try to preserve syntactical and semantic information. The methods such as Bag of Words(BOW), CountVectorizer and TFIDF rely on the word count in a sentence but do not save any syntactical or semantic information. In these algorithms, the size of the vector is the number of elements in the vocabulary. We can get a sparse matrix if most of the elements are zero. Large input vectors will mean a huge number of weights which will result in high computation required for training. Word Embeddings give a solution to these problems."],"metadata":{"id":"Rv73CVJgRpuB"}},{"cell_type":"markdown","source":["---\n","---"],"metadata":{"id":"Wql1VJlORzZ4"}}]}